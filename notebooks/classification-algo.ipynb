{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0045602",
   "metadata": {},
   "source": [
    "# Machine Learning Experimentation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Hypothesis\n",
    "Start with a clear hypothesis: What are you trying to prove or discover about the algorithms (Neural Network, KNN, and SVM) across two different datasets? Outline your experimental steps to test this hypothesis.\n",
    "\n",
    "### 1.2 Datasets\n",
    "Introduce your datasets:\n",
    "- **Brain Tumor Classification MRI Dataset:**\n",
    "  - **Description:** This dataset contains MRI images of brain tumors, classified into different types. It’s interesting from an ML perspective due to its medical application and the challenge of accurately classifying tumor types based on image data.\n",
    "  - **Preprocessing:** Discuss any preprocessing steps you applied, such as image resizing, normalization, or data augmentation.\n",
    "  - **Source:** [Kaggle Brain Tumor Classification MRI Dataset](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data)\n",
    "\n",
    "- **Cervical Cancer Risk Classification Dataset:**\n",
    "  - **Description:** This dataset includes risk factors for cervical cancer, which is valuable for predictive modeling in healthcare. It’s challenging due to potential class imbalances and the importance of precision in medical diagnostics.\n",
    "  - **Preprocessing:** Mention any preprocessing steps, like handling missing values, feature scaling, or encoding categorical variables.\n",
    "  - **Source:** [Kaggle Cervical Cancer Risk Classification Dataset](https://www.kaggle.com/datasets/loveall/cervical-cancer-risk-classification?select=kag_risk_factors_cervical_cancer.csv)\n",
    "\n",
    "### 1.3 Experimental Methodology\n",
    "Discuss your overall methodology:\n",
    "- **Training and Test Sets:** The datasets are pre-split into training and test sets.\n",
    "- **Validation Split:** Set aside the test set and further split your training set into training and validation sets for each dataset.\n",
    "- **Reproducibility:** Ensure results are reproducible by setting seeds (e.g., using `random_state` or `numpy.random.seed`).\n",
    "\n",
    "## 2. Model Training and Tuning\n",
    "\n",
    "### 2.1 Algorithms\n",
    "Describe the three algorithms you are testing:\n",
    "- **Neural Network**\n",
    "- **K-Nearest Neighbors (KNN)**\n",
    "- **Support Vector Machine (SVM)**\n",
    "\n",
    "### 2.2 Cross-Validation (CV)\n",
    "Use cross-validation to train your models for each algorithm across both datasets, providing a more robust estimate of model performance.\n",
    "\n",
    "### 2.3 Learning and Validation Curves\n",
    "Analyze the bias-variance trade-off using learning curves, and identify overfitting or underfitting with validation curves for each algorithm.\n",
    "\n",
    "### 2.4 Hyperparameter Tuning\n",
    "Discuss the process of hyperparameter tuning for each algorithm:\n",
    "- Use techniques like grid search or random search.\n",
    "- Adjust hyperparameters iteratively, using the validation set.\n",
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "### 3.1 Final Model Evaluation\n",
    "Once you’re satisfied with your model's performance, evaluate it on the test set, which has been untouched during the training and tuning phases.\n",
    "\n",
    "### 3.2 Performance Metrics\n",
    "Evaluate your models using various performance metrics:\n",
    "- For balanced data: Accuracy, Precision, Recall, F1 Score.\n",
    "- For imbalanced data: Precision, Recall, F1 Score, ROC Curves, PRAUC Curves, Confusion Matrices, Decision Surfaces.\n",
    "\n",
    "## 4. Results and Discussion\n",
    "\n",
    "### 4.1 Isolated Algorithm Results\n",
    "Present results isolated to each algorithm and dataset:\n",
    "- How did each algorithm perform on each dataset?\n",
    "- Any specific observations about algorithmic behavior or hyperparameter interaction?\n",
    "\n",
    "### 4.2 Comparative Analysis\n",
    "Compare and contrast the results across algorithms and datasets:\n",
    "- Discuss how Neural Network, KNN, and SVM performed across both datasets.\n",
    "- Highlight any notable differences or similarities.\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "### 5.1 Summary and Learnings\n",
    "Conclude by summarizing your findings:\n",
    "- Reflect on your hypothesis and whether it was supported.\n",
    "- Link the results back to algorithmic behavior, hyperparameter interactions, and input data.\n",
    "- Include insights gained from lectures that applied to this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362777c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097f7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Score:  0.29094076655052264\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = '/home/ec2-user/SageMaker/ML-Algorithm-Benchmarks/data/brain_tumor/Training'\n",
    "\n",
    "# Prepare data containers\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Define image size and batch size\n",
    "image_size = (128, 128)\n",
    "batch_size = 128\n",
    "\n",
    "# Loop over each label which is a folder in this case\n",
    "for label in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, label)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for image_name in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize(image_size)\n",
    "            image = np.array(image) / 255.0\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Shuffle and split data into training and test sets\n",
    "X, y_encoded = shuffle(X, y_encoded, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1, warm_start=True, random_state=42)\n",
    "\n",
    "# Function to train the model in batches\n",
    "def train_in_batches(model, X_train, y_train, batch_size):\n",
    "    num_samples = X_train.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Select a batch\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        # Fit the model on the batch\n",
    "        model.fit(X_batch.reshape(X_batch.shape[0], -1), y_batch)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model on the training data in batches\n",
    "model = train_in_batches(model, X_train, y_train, batch_size)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score = model.score(X_test.reshape(X_test.shape[0], -1), y_test)\n",
    "print(\"Test Set Score: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db11838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d2055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
